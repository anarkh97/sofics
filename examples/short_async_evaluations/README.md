# Short Ansychronous Evaluation Case

This example case demonstrates the simulation setup required for successful evaluation of two structural design configurations using coupled fluid-structure interaction simulations. Each simulation in this case utilizes 64 computational cores: 56 for the fluid solver and 8 for the structural solver. Simulations will be executed asynchronously on a single compute node, through the `SLURM` scheduling directive.

We consider an ellipsoidal containment structure which is subjected to an internal TNT explosion. The major and minor radii of the containment structure serve as the variable parameters in the two evaluations. The geometry of the containment structure is defined in the `templates/struct.geo.template` file using `Gmsh`'s dedicated `.geo` language. In this template, placeholders like `{cdv_1}` and `{cdv_2}` represent the major and minor radii values. `SOFICS` automatically replaces these placeholders with the appropriate values from the parameter file generated by `Dakota`.

The generic Aero-S and M2C simulation setups are defined in the `templates/fem.in.template` and `templates/input.st.template`, respectively. The non-linear detonation-induced pressure, density, and velocity profile is provided in the `templates/SphericalShock.txt.template` which serves as an input for `M2C`.

<!-- ## Structural simualtion setup -->

<!-- ## Fluid simulation setup -->

## Dakota Setup

Asynchronous evaluations are spwaned using `Dakota`'s `fork` application interface, with the required setup specified in `dakota.in` input file. The `fork` interface requires an `analysis_driver` that reads the provided desgin parameters, performs the neccessary evaluations, and outputs the response functions. In `SOFICS`, the `driver.sh` bash script located in your `build` directory serves as the `analysis_dirver`. This script requires user-defined setup details, including input files for `Gmsh`, `M2C`, and `Aero-S`, as well as resource specifications for each evaluation. These details are supplied to `driver.sh` via a configuration file, similar to the example `myconfig.sh` provided. The required parameters to be defined in this configuration file are listed below.

* Fluid simulation setup
    * ***M2C_INPUT***: Should be set to the name of the `M2C` input file that contains the configuration for the fluid simulation. The file must contain `under ConcurrentPrograms { under AeroS { FSIAlgorithm = ByAeroS; }}` which instructs `M2C` to treat the simulation as a coupled fluid-structure interaction simulation.
    * ***M2C_AUX***: Should be a colon-delimited list of additional files required by `M2C` for the fluid simulation, e.g., `file1:file2:file3`.
    * ***M2C_EXE***: Should be set to the path of your personal `M2C` executable. By default, `SOFICS` uses the locally packaged version of `M2C` if available; otherwise, an error will be raised.
    * ***M2C_SIZE***: Should be set to the number of computational cores allocated to `M2C` for each coupled fluid-structure interaction simulation.
* Structural simulation setup
    * ***AEROS_INPUT***: Should be set to the name of the `Aero-S` input file that contains the configuration for the structural simulation. The file must contain `EMBEDDED #` card, where `#` is replaced with the surface ID of the embedded or the wetted surface as defined in the `Gmsh` input file. This card instructs `Aero-S` to treat the simulation as a coupled fluid-structure interaction simulation.
    * ***AEROS_EXE***: Should be set to the path of your personal `Aero-S` executable. By default, `SOFICS` uses the locally packaged version of `Aero-S` if available; otherwise, an error will be raised.
    * ***AEROS_SIZE***: Should be set to the number of computational cores allocated to `Aero-S` for each coupled fluid-structure interaction simulation.
* Finite-element meshing setup
    * ***GMSH_INPUT***: Should be set to the name of the `Gmsh` input file that defines the procedure for building and meshing the structural geometry. The continuous design variables provided by `Dakota` can be used in this file by wrapping the variable name in `{}`. `SOFICS` uses the default variable names, where continuous design variables follow the pattern `cdv_i`, and continuous state variables follow `csv_i`.
    * ***GMSH_EXE***: Should be set to the path of your personal `Gmsh` executable. If `Gmsh` is installed correctly, this variable can simply be `GMSH_EXE=gmsh`.
* Miscellaneous
    * ***TEMPLATE_DIR***: Should be the name of the directory where you store all the input file templates, if you choose to organize them in one location. By default, `SOFICS` uses the directory from which the `Dakota` job is launched.
    * ***EVALUATION_CONCURRENCY***: Should reflect the evaluation concurrency specified in the `Dakota` input file.

## Evaluation

The `SLURM` scheduller is employed to launch the `Dakota` process on Virginia Tech's `Tinkercliffs` compute cluster. An example `SLURM` configuration can be found in the `run.sh` file. Update the following lines to match your preference and account details:

```sh
#SBATCH --job-name=dakota           # Job name
#SBATCH --partition=normal_q        # Partition or queue name
#SBATCH --account=m2clab            # Cluster account
```

The script uses the `dakota` command to call your `Dakota` installation, so ensure `Dakota` is properly installed before submitting a job. Follow the installation instructions available on the official [Dakota repository](https://github.com/snl-dakota/dakota?tab=coc-ov-file).

To submit a job on the compute cluster, use:

```sh
sbatch run.sh
```

To verify that the job was successfully submitted, run:

```sh
squeue | grep "your-user-id"
```

This command will display a list of jobs currently running under your user ID on the cluster.

## Expectation

The test case is designed to run for 10 minutes, and will terminate the coupled fluid-structure interaction evaluations abruptly. It is specifically tailored to demonstrate the required inputs and the procedure for submitting a `Dakota` job to the compute cluster. Upon completion, your directory should contain two new subdirectories along with log files generated by `Dakota`. 

To confirm that `Dakota` successfully spwaned the coupled fluid-structure interaction processes, use:

```sh
grep "Evaluation" srun.out
```

This command will display the evaluation IDs along with the compute node IDs where each evaluation was executed. These should align with the node assigned to the `Dakota` job (refer the `squeue` command output), as shown in the example below:

![](../../images/SOFICSOutputVerification.jpg)

To verify the coupled fluid-structure interaction processes launched correctly, use:

```sh
tail evaluation.1/log.out
```

The command output from a successful execution should resemble,

![](../../images/FsiLogOutput.jpg)