# Full Genetic Optimization of Ellipsoidal Blast Containment Structures

This example case demonstrates the setup required for optimizing blast containment structures using coupled fluid-structure interaction simulations. Each simulation in this case utilizes 64 computational cores: 56 for the fluid solver and 8 for the structural solver. Simulations will be executed asynchronously on multiple compute nodes, through the `SLURM` scheduling directive.

We analyze an ellipsoidal containment structure subjected to an internal TNT explosion. The major and minor radii of the containment structure are treated as the design variables in the optimization study. The goal is to minimize the mass of the blast container while ensuring that the maximum effective plastic strain remains below a specified limit. Mathematically, this problem is formulated as:

$$
\min_{x_{dv} \in \mathbb{R}^n} f\big(x_{dv}\big)
$$

Subject to:

$$
e_{fluid}(x_{dv}, u) = 0\\
e_{structure}(x_{dv}, u) = 0\\
\epsilon_p \leq \epsilon^{crit}_p\\
$$

where $x_{dv}$ and $u$ denote the design variables and the state variables, respectively. $e_{fluid}$ and $e_{structure}$ represent the governing partial differencial equations for fluid dynamics and structural dynamics. $\epsilon_p$ is the effective plastic strain developed within the structure, while $\epsilon^{crit}_p$ is the imposed critical limit value. In this demonstration case, $f(x_{dv})$ represents the mass of the containment structure, which serves as the optimization objective.

This optimization study employs the Genetic Algorithm implemented in `Dakota`. This data-driven approach does not require gradient computations for iterative updates to the blast containment structure, making it particularly advantageous. These methods are inherently parallel, enabling each fluid-structure simulation to run asynchronously across multiple compute nodes, thereby improving computational efficiency.

The geometry of the containment structure is defined in the `templates/struct.geo.template` file using `Gmsh`'s dedicated `.geo` language. In this template, placeholders like `{cdv_1}` and `{cdv_2}` represent the major and minor radii values. `SOFICS` automatically replaces these placeholders with the appropriate values from the parameter file generated by `Dakota`.

The generic Aero-S and M2C simulation setups are defined in the `templates/fem.in.template` and `templates/input.st.template`, respectively. The non-linear detonation-induced pressure, density, and velocity profile is provided in the `templates/SphericalShock.txt.template` which serves as an input for `M2C`.

<!-- ## Structural simualtion setup -->

<!-- ## Fluid simulation setup -->

## Dakota Setup

Asynchronous evaluations are spwaned using `Dakota`'s `fork` application interface, with the required setup specified in `dakota.in` input file. The `fork` interface requires an `analysis_driver` that reads the provided desgin parameters, performs the neccessary evaluations, and outputs the response functions. In `SOFICS`, the `driver.sh` bash script located in your `build` directory serves as the `analysis_dirver`. This script requires user-defined setup details, including input files for `Gmsh`, `M2C`, and `Aero-S`, as well as resource specifications for each evaluation. These details are supplied to `driver.sh` via a configuration file, similar to the example `config.sh` provided. 

<!-- The required parameters to be defined in this configuration file are listed below.

* Fluid simulation setup
    * ***M2C_INPUT***: Should be set to the name of the `M2C` input file that contains the configuration for the fluid simulation. The file must contain `under ConcurrentPrograms { under AeroS { FSIAlgorithm = ByAeroS; }}` which instructs `M2C` to treat the simulation as a coupled fluid-structure interaction simulation.
    * ***M2C_AUX***: Should be a colon-delimited list of additional files required by `M2C` for the fluid simulation, e.g., `file1:file2:file3`.
    * ***M2C_EXE***: Should be set to the path of your personal `M2C` executable. By default, `SOFICS` uses the locally packaged version of `M2C` if available; otherwise, an error will be raised.
    * ***M2C_SIZE***: Should be set to the number of computational cores allocated to `M2C` for each coupled fluid-structure interaction simulation.
* Structural simulation setup
    * ***AEROS_INPUT***: Should be set to the name of the `Aero-S` input file that contains the configuration for the structural simulation. The file must contain `EMBEDDED #` card, where `#` is replaced with the surface ID of the embedded or the wetted surface as defined in the `Gmsh` input file. This card instructs `Aero-S` to treat the simulation as a coupled fluid-structure interaction simulation.
    * ***AEROS_EXE***: Should be set to the path of your personal `Aero-S` executable. By default, `SOFICS` uses the locally packaged version of `Aero-S` if available; otherwise, an error will be raised.
    * ***AEROS_SIZE***: Should be set to the number of computational cores allocated to `Aero-S` for each coupled fluid-structure interaction simulation.
* Finite-element meshing setup
    * ***GMSH_INPUT***: Should be set to the name of the `Gmsh` input file that defines the procedure for building and meshing the structural geometry. The continuous design variables provided by `Dakota` can be used in this file by wrapping the variable name in `{}`. `SOFICS` uses the default variable names, where continuous design variables follow the pattern `cdv_i`, and continuous state variables follow `csv_i`.
    * ***GMSH_EXE***: Should be set to the path of your personal `Gmsh` executable. If `Gmsh` is installed correctly, this variable can simply be `GMSH_EXE=gmsh`.
* Miscellaneous
    * ***TEMPLATE_DIR***: Should be the name of the directory where you store all the input file templates, if you choose to organize them in one location. By default, `SOFICS` uses the directory from which the `Dakota` job is launched.
    * ***EVALUATION_CONCURRENCY***: Should reflect the evaluation concurrency specified in the `Dakota` input file. -->

## Evaluation

The `SLURM` scheduller is employed to launch the `Dakota` process on Virginia Tech's `Tinkercliffs` compute cluster. An example `SLURM` configuration can be found in the `run.sh` file. Update the following lines to match your preference and account details:

```sh
#SBATCH --job-name=dakota           # Job name
#SBATCH --partition=normal_q        # Partition or queue name
#SBATCH --account=m2clab            # Cluster account
```

The script uses the `dakota` command to call your `Dakota` installation, so ensure `Dakota` is properly installed before submitting a job. Follow the installation instructions available on the official [Dakota repository](https://github.com/snl-dakota/dakota?tab=coc-ov-file).

***Note:*** Ensure that sufficient compute nodes are allocated to the job. In this demonstration, each simulation requires 64 computational cores (CPUs). Therefore, the total number of cores needed will be `64 × evaluation concurrency`. Since each node on `Tinkercliffs` consists of 128 CPUs, you should adjust your resource allocation accordingly. Update the following line in `run.sh` to specify your resource requirements:
```sh
#SBATCH --nodes=4                   # Number of nodes
#SBATCH --ntasks-per-node=128       # Number of tasks per node
```

To submit a job on the compute cluster, use:

```sh
sbatch run.sh
```

To verify that the job was successfully submitted, run:

```sh
squeue | grep "your-user-id"
```

This command will display a list of jobs currently running under your user ID on the cluster.

## Results

